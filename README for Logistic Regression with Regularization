# ğŸ¤– Polynomial Logistic Regression from Scratch with Regularization

This project demonstrates how to implement **Polynomial Logistic Regression from scratch** using **NumPy** and **matplotlib**, with **L2 regularization** to reduce overfitting. It uses synthetically generated 2D binary classification data.

---

## ğŸ“Œ Project Overview

- Logistic Regression (binary classification)
- Polynomial feature expansion up to 3rd degree
- Gradient Descent optimization
- L2 Regularization (Ridge)
- Visualization of:
  - Cost reduction over iterations
  - Decision boundary (non-linear)
- No external ML libraries (fully manual implementation)

---

## ğŸ§ª Dataset

Generated using `sklearn.datasets.make_classification`:

- **Samples**: 200  
- **Features**: 2  
- **Classes**: 2  
- **Noise**: 10% label flipping  
- **Cluster separation**: 0.8

---

## ğŸ§  Polynomial Feature Engineering

We manually construct polynomial features from the two normalized input features:

- First-order: `x1, x2`  
- Second-order: `x1Â², x2Â², x1 * x2`  
- Third-order: `x1Â³, x2Â³`

Final shape of training data: **(200, 7)**

---

## âš™ï¸ Model Training (Logistic Regression)

### ğŸ”¸ Initialization

- Parameters: `w` (weights), `b` (bias) initialized to 0
- Learning rate: `alpha = 0.01`
- Regularization strength: `Î» = 0.01`
- Iterations: `50000`

### ğŸ”¸ Training Loop

1. **Forward pass**: compute `z = X_poly @ w + b`
2. Apply **sigmoid activation**:  
   \[
   y_{\text{pred}} = \frac{1}{1 + e^{-z}}
   \]
3. Compute **regularized binary cross-entropy loss**:
   \[
   J = - \frac{1}{m} \sum \left[ y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \right] + \frac{\lambda}{2m} \sum w^2
   \]
4. Compute gradients with regularization
5. Update `w` and `b` using gradient descent

---

## ğŸ“‰ Cost Reduction Plot

The cost consistently decreases over 50,000 iterations.  
**Final cost**: `0.4878`, indicating good model fit.

---

## ğŸ§© Decision Boundary Visualization

- We use a mesh grid over the feature space
- Predict probabilities for each grid point using the trained model
- Draw the boundary where predicted probability = 0.5

- ğŸ”µ Class 1 points  
- ğŸ”´ Class 0 points  
- ğŸŒŠ **Cyan contour line**: predicted decision boundary

---

## ğŸ“Š Final Results

| Component           | Value             |
|---------------------|------------------|
| Samples             | 200              |
| Features (after poly) | 7               |
| Final cost          | 0.4878           |
| Regularization      | L2 (Î» = 0.01)     |
| Boundary            | Curved / Non-linear |

--

